{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T16:58:49.373562Z",
     "start_time": "2021-03-05T16:58:49.370529Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:19:26.761140Z",
     "start_time": "2021-03-05T17:19:26.758105Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:19:27.158918Z",
     "start_time": "2021-03-05T17:19:27.151915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Input',\n",
       " 'Model',\n",
       " 'Sequential',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_sys',\n",
       " 'activations',\n",
       " 'applications',\n",
       " 'backend',\n",
       " 'callbacks',\n",
       " 'constraints',\n",
       " 'datasets',\n",
       " 'estimator',\n",
       " 'experimental',\n",
       " 'initializers',\n",
       " 'layers',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'mixed_precision',\n",
       " 'models',\n",
       " 'optimizers',\n",
       " 'preprocessing',\n",
       " 'regularizers',\n",
       " 'utils',\n",
       " 'wrappers']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:19:27.872716Z",
     "start_time": "2021-03-05T17:19:27.868688Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import models,layers,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:22:28.152804Z",
     "start_time": "2021-03-05T17:22:27.906980Z"
    }
   },
   "outputs": [],
   "source": [
    "(train_images,train_labels),(test_images,test_labels)=datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:23:14.077898Z",
     "start_time": "2021-03-05T17:23:14.071942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:24:51.317164Z",
     "start_time": "2021-03-05T17:24:51.314132Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:51:48.740291Z",
     "start_time": "2021-03-05T17:51:48.734307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:08.877317Z",
     "start_time": "2021-03-05T17:52:08.871336Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names=['one','two','three','four','five','six','seven','eight','nine','ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:09.349853Z",
     "start_time": "2021-03-05T17:52:09.344866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the data\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:09.617303Z",
     "start_time": "2021-03-05T17:52:09.612330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:09.827720Z",
     "start_time": "2021-03-05T17:52:09.822734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:10.142264Z",
     "start_time": "2021-03-05T17:52:10.138238Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:10.839627Z",
     "start_time": "2021-03-05T17:52:10.713004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2133cf09188>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:11.293257Z",
     "start_time": "2021-03-05T17:52:11.289267Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:52:11.849499Z",
     "start_time": "2021-03-05T17:52:11.841525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractRNNCell',\n",
       " 'Activation',\n",
       " 'ActivityRegularization',\n",
       " 'Add',\n",
       " 'AdditiveAttention',\n",
       " 'AlphaDropout',\n",
       " 'Attention',\n",
       " 'Average',\n",
       " 'AveragePooling1D',\n",
       " 'AveragePooling2D',\n",
       " 'AveragePooling3D',\n",
       " 'AvgPool1D',\n",
       " 'AvgPool2D',\n",
       " 'AvgPool3D',\n",
       " 'BatchNormalization',\n",
       " 'Bidirectional',\n",
       " 'Concatenate',\n",
       " 'Conv1D',\n",
       " 'Conv1DTranspose',\n",
       " 'Conv2D',\n",
       " 'Conv2DTranspose',\n",
       " 'Conv3D',\n",
       " 'Conv3DTranspose',\n",
       " 'ConvLSTM2D',\n",
       " 'Convolution1D',\n",
       " 'Convolution1DTranspose',\n",
       " 'Convolution2D',\n",
       " 'Convolution2DTranspose',\n",
       " 'Convolution3D',\n",
       " 'Convolution3DTranspose',\n",
       " 'Cropping1D',\n",
       " 'Cropping2D',\n",
       " 'Cropping3D',\n",
       " 'Dense',\n",
       " 'DenseFeatures',\n",
       " 'DepthwiseConv2D',\n",
       " 'Dot',\n",
       " 'Dropout',\n",
       " 'ELU',\n",
       " 'Embedding',\n",
       " 'Flatten',\n",
       " 'GRU',\n",
       " 'GRUCell',\n",
       " 'GaussianDropout',\n",
       " 'GaussianNoise',\n",
       " 'GlobalAveragePooling1D',\n",
       " 'GlobalAveragePooling2D',\n",
       " 'GlobalAveragePooling3D',\n",
       " 'GlobalAvgPool1D',\n",
       " 'GlobalAvgPool2D',\n",
       " 'GlobalAvgPool3D',\n",
       " 'GlobalMaxPool1D',\n",
       " 'GlobalMaxPool2D',\n",
       " 'GlobalMaxPool3D',\n",
       " 'GlobalMaxPooling1D',\n",
       " 'GlobalMaxPooling2D',\n",
       " 'GlobalMaxPooling3D',\n",
       " 'Input',\n",
       " 'InputLayer',\n",
       " 'InputSpec',\n",
       " 'LSTM',\n",
       " 'LSTMCell',\n",
       " 'Lambda',\n",
       " 'Layer',\n",
       " 'LayerNormalization',\n",
       " 'LeakyReLU',\n",
       " 'LocallyConnected1D',\n",
       " 'LocallyConnected2D',\n",
       " 'Masking',\n",
       " 'MaxPool1D',\n",
       " 'MaxPool2D',\n",
       " 'MaxPool3D',\n",
       " 'MaxPooling1D',\n",
       " 'MaxPooling2D',\n",
       " 'MaxPooling3D',\n",
       " 'Maximum',\n",
       " 'Minimum',\n",
       " 'Multiply',\n",
       " 'PReLU',\n",
       " 'Permute',\n",
       " 'RNN',\n",
       " 'ReLU',\n",
       " 'RepeatVector',\n",
       " 'Reshape',\n",
       " 'SeparableConv1D',\n",
       " 'SeparableConv2D',\n",
       " 'SeparableConvolution1D',\n",
       " 'SeparableConvolution2D',\n",
       " 'SimpleRNN',\n",
       " 'SimpleRNNCell',\n",
       " 'Softmax',\n",
       " 'SpatialDropout1D',\n",
       " 'SpatialDropout2D',\n",
       " 'SpatialDropout3D',\n",
       " 'StackedRNNCells',\n",
       " 'Subtract',\n",
       " 'ThresholdedReLU',\n",
       " 'TimeDistributed',\n",
       " 'UpSampling1D',\n",
       " 'UpSampling2D',\n",
       " 'UpSampling3D',\n",
       " 'Wrapper',\n",
       " 'ZeroPadding1D',\n",
       " 'ZeroPadding2D',\n",
       " 'ZeroPadding3D',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'add',\n",
       " 'average',\n",
       " 'concatenate',\n",
       " 'deserialize',\n",
       " 'dot',\n",
       " 'experimental',\n",
       " 'maximum',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'serialize',\n",
       " 'subtract']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir(tensorflow.keras.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T18:04:32.266061Z",
     "start_time": "2021-03-05T18:04:16.221952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 8.5322 - accuracy: 0.12 - ETA: 2s - loss: 12.9101 - accuracy: 0.096 - ETA: 2s - loss: 13.6349 - accuracy: 0.104 - ETA: 2s - loss: 13.8848 - accuracy: 0.104 - ETA: 1s - loss: 13.9916 - accuracy: 0.107 - ETA: 2s - loss: 14.0819 - accuracy: 0.106 - ETA: 2s - loss: 14.1653 - accuracy: 0.103 - ETA: 1s - loss: 14.2137 - accuracy: 0.103 - ETA: 1s - loss: 14.2355 - accuracy: 0.104 - ETA: 1s - loss: 14.3050 - accuracy: 0.101 - ETA: 1s - loss: 14.3206 - accuracy: 0.102 - ETA: 1s - loss: 14.3437 - accuracy: 0.101 - ETA: 1s - loss: 14.3634 - accuracy: 0.101 - ETA: 1s - loss: 14.3718 - accuracy: 0.101 - ETA: 1s - loss: 14.3830 - accuracy: 0.101 - ETA: 1s - loss: 14.3799 - accuracy: 0.100 - ETA: 1s - loss: 14.3128 - accuracy: 0.100 - ETA: 1s - loss: 13.7800 - accuracy: 0.103 - ETA: 1s - loss: 13.1473 - accuracy: 0.108 - ETA: 1s - loss: 12.5935 - accuracy: 0.112 - ETA: 1s - loss: 12.0731 - accuracy: 0.116 - ETA: 0s - loss: 11.5382 - accuracy: 0.119 - ETA: 0s - loss: 11.1059 - accuracy: 0.119 - ETA: 0s - loss: 10.7124 - accuracy: 0.119 - ETA: 0s - loss: 10.3454 - accuracy: 0.119 - ETA: 0s - loss: 9.9580 - accuracy: 0.119 - ETA: 0s - loss: 9.6528 - accuracy: 0.11 - ETA: 0s - loss: 9.4036 - accuracy: 0.11 - ETA: 0s - loss: 9.2068 - accuracy: 0.11 - ETA: 0s - loss: 8.9965 - accuracy: 0.11 - ETA: 0s - loss: 8.8214 - accuracy: 0.11 - ETA: 0s - loss: 8.6334 - accuracy: 0.11 - ETA: 0s - loss: 8.4765 - accuracy: 0.11 - ETA: 0s - loss: 8.3115 - accuracy: 0.11 - ETA: 0s - loss: 8.1590 - accuracy: 0.11 - ETA: 0s - loss: 8.0210 - accuracy: 0.11 - ETA: 0s - loss: 7.8556 - accuracy: 0.11 - ETA: 0s - loss: 7.6807 - accuracy: 0.11 - ETA: 0s - loss: 7.5253 - accuracy: 0.11 - ETA: 0s - loss: 7.3425 - accuracy: 0.11 - 2s 1ms/step - loss: 7.2189 - accuracy: 0.1164\n",
      "Epoch 2/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.06 - ETA: 2s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3040 - accuracy: 0.11 - ETA: 1s - loss: 2.3038 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3034 - accuracy: 0.11 - ETA: 1s - loss: 2.3033 - accuracy: 0.11 - ETA: 1s - loss: 2.3033 - accuracy: 0.11 - ETA: 1s - loss: 2.3032 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 945us/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 3/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.03 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3033 - accuracy: 0.10 - ETA: 1s - loss: 2.3033 - accuracy: 0.10 - ETA: 1s - loss: 2.3039 - accuracy: 0.10 - ETA: 1s - loss: 2.3038 - accuracy: 0.10 - ETA: 1s - loss: 2.3038 - accuracy: 0.10 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 1ms/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 4/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.15 - ETA: 2s - loss: 2.3026 - accuracy: 0.12 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3034 - accuracy: 0.11 - ETA: 1s - loss: 2.3041 - accuracy: 0.11 - ETA: 1s - loss: 2.3040 - accuracy: 0.11 - ETA: 1s - loss: 2.3039 - accuracy: 0.11 - ETA: 1s - loss: 2.3038 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3040 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 1ms/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.09 - ETA: 1s - loss: 2.3026 - accuracy: 0.12 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 1s - loss: 2.3051 - accuracy: 0.11 - ETA: 1s - loss: 2.3045 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3040 - accuracy: 0.11 - ETA: 1s - loss: 2.3038 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3034 - accuracy: 0.11 - ETA: 1s - loss: 2.3034 - accuracy: 0.11 - ETA: 1s - loss: 2.3033 - accuracy: 0.11 - ETA: 1s - loss: 2.3033 - accuracy: 0.11 - ETA: 1s - loss: 2.3032 - accuracy: 0.11 - ETA: 1s - loss: 2.3032 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3035 - accuracy: 0.11 - ETA: 1s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 1ms/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 6/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.09 - ETA: 2s - loss: 2.3026 - accuracy: 0.10 - ETA: 2s - loss: 2.3026 - accuracy: 0.11 - ETA: 2s - loss: 2.3060 - accuracy: 0.10 - ETA: 2s - loss: 2.3051 - accuracy: 0.11 - ETA: 1s - loss: 2.3045 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3039 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3046 - accuracy: 0.11 - ETA: 1s - loss: 2.3044 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3041 - accuracy: 0.11 - ETA: 1s - loss: 2.3040 - accuracy: 0.11 - ETA: 1s - loss: 2.3039 - accuracy: 0.11 - ETA: 1s - loss: 2.3038 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3036 - accuracy: 0.11 - ETA: 1s - loss: 2.3041 - accuracy: 0.11 - ETA: 0s - loss: 2.3040 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 1ms/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 7/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.03 - ETA: 2s - loss: 2.3026 - accuracy: 0.13 - ETA: 2s - loss: 2.3078 - accuracy: 0.12 - ETA: 2s - loss: 2.3060 - accuracy: 0.11 - ETA: 2s - loss: 2.3051 - accuracy: 0.11 - ETA: 1s - loss: 2.3045 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3039 - accuracy: 0.11 - ETA: 1s - loss: 2.3049 - accuracy: 0.11 - ETA: 1s - loss: 2.3046 - accuracy: 0.11 - ETA: 1s - loss: 2.3044 - accuracy: 0.11 - ETA: 1s - loss: 2.3043 - accuracy: 0.11 - ETA: 1s - loss: 2.3041 - accuracy: 0.11 - ETA: 1s - loss: 2.3040 - accuracy: 0.11 - ETA: 1s - loss: 2.3045 - accuracy: 0.11 - ETA: 1s - loss: 2.3044 - accuracy: 0.11 - ETA: 1s - loss: 2.3043 - accuracy: 0.11 - ETA: 1s - loss: 2.3042 - accuracy: 0.11 - ETA: 1s - loss: 2.3046 - accuracy: 0.11 - ETA: 0s - loss: 2.3045 - accuracy: 0.11 - ETA: 0s - loss: 2.3043 - accuracy: 0.11 - ETA: 0s - loss: 2.3042 - accuracy: 0.11 - ETA: 0s - loss: 2.3042 - accuracy: 0.11 - ETA: 0s - loss: 2.3041 - accuracy: 0.11 - ETA: 0s - loss: 2.3040 - accuracy: 0.11 - ETA: 0s - loss: 2.3040 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3039 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3038 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 1ms/step - loss: 2.3035 - accuracy: 0.1119\n",
      "Epoch 8/8\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.09 - ETA: 1s - loss: 2.3026 - accuracy: 0.11 - ETA: 2s - loss: 2.3026 - accuracy: 0.10 - ETA: 2s - loss: 2.3026 - accuracy: 0.10 - ETA: 2s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3026 - accuracy: 0.10 - ETA: 1s - loss: 2.3041 - accuracy: 0.10 - ETA: 1s - loss: 2.3039 - accuracy: 0.10 - ETA: 1s - loss: 2.3037 - accuracy: 0.10 - ETA: 1s - loss: 2.3035 - accuracy: 0.10 - ETA: 1s - loss: 2.3034 - accuracy: 0.10 - ETA: 1s - loss: 2.3033 - accuracy: 0.10 - ETA: 1s - loss: 2.3033 - accuracy: 0.11 - ETA: 1s - loss: 2.3038 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 1s - loss: 2.3037 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3036 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3034 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3032 - accuracy: 0.11 - ETA: 0s - loss: 2.3031 - accuracy: 0.11 - ETA: 0s - loss: 2.3031 - accuracy: 0.11 - ETA: 0s - loss: 2.3031 - accuracy: 0.11 - ETA: 0s - loss: 2.3031 - accuracy: 0.11 - ETA: 0s - loss: 2.3033 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - ETA: 0s - loss: 2.3035 - accuracy: 0.11 - 2s 970us/step - loss: 2.3035 - accuracy: 0.1119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2133ea09788>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "model=Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)]\n",
    "    )\n",
    "\n",
    "#model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics='Accuracy')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='SparseCategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,epochs=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T18:07:39.002068Z",
     "start_time": "2021-03-05T18:06:42.915890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 6.2281 - accuracy: 0.12 - ETA: 3s - loss: 9.0537 - accuracy: 0.09 - ETA: 4s - loss: 9.9891 - accuracy: 0.10 - ETA: 3s - loss: 10.5224 - accuracy: 0.108 - ETA: 3s - loss: 10.5200 - accuracy: 0.114 - ETA: 3s - loss: 10.4399 - accuracy: 0.122 - ETA: 3s - loss: 10.6344 - accuracy: 0.119 - ETA: 3s - loss: 10.9174 - accuracy: 0.115 - ETA: 3s - loss: 11.1127 - accuracy: 0.114 - ETA: 3s - loss: 11.2375 - accuracy: 0.111 - ETA: 3s - loss: 11.3658 - accuracy: 0.109 - ETA: 3s - loss: 11.4939 - accuracy: 0.108 - ETA: 2s - loss: 11.6072 - accuracy: 0.106 - ETA: 2s - loss: 11.7549 - accuracy: 0.105 - ETA: 2s - loss: 11.8452 - accuracy: 0.104 - ETA: 2s - loss: 11.8866 - accuracy: 0.103 - ETA: 2s - loss: 11.8992 - accuracy: 0.104 - ETA: 2s - loss: 11.9267 - accuracy: 0.103 - ETA: 2s - loss: 11.9629 - accuracy: 0.102 - ETA: 2s - loss: 11.9849 - accuracy: 0.102 - ETA: 2s - loss: 12.0080 - accuracy: 0.102 - ETA: 2s - loss: 12.0189 - accuracy: 0.103 - ETA: 2s - loss: 12.0533 - accuracy: 0.102 - ETA: 2s - loss: 12.0797 - accuracy: 0.102 - ETA: 2s - loss: 12.1063 - accuracy: 0.101 - ETA: 2s - loss: 12.1233 - accuracy: 0.102 - ETA: 1s - loss: 12.1437 - accuracy: 0.102 - ETA: 1s - loss: 12.1684 - accuracy: 0.102 - ETA: 1s - loss: 12.1862 - accuracy: 0.101 - ETA: 1s - loss: 12.2059 - accuracy: 0.102 - ETA: 1s - loss: 12.2144 - accuracy: 0.102 - ETA: 1s - loss: 12.2157 - accuracy: 0.102 - ETA: 1s - loss: 12.2371 - accuracy: 0.102 - ETA: 1s - loss: 12.2551 - accuracy: 0.102 - ETA: 1s - loss: 12.2602 - accuracy: 0.101 - ETA: 1s - loss: 12.2731 - accuracy: 0.101 - ETA: 1s - loss: 12.2792 - accuracy: 0.101 - ETA: 1s - loss: 12.2809 - accuracy: 0.101 - ETA: 1s - loss: 12.2864 - accuracy: 0.101 - ETA: 1s - loss: 12.3020 - accuracy: 0.101 - ETA: 1s - loss: 12.3111 - accuracy: 0.100 - ETA: 1s - loss: 12.3157 - accuracy: 0.100 - ETA: 1s - loss: 12.3201 - accuracy: 0.100 - ETA: 1s - loss: 12.3339 - accuracy: 0.100 - ETA: 1s - loss: 12.3365 - accuracy: 0.100 - ETA: 1s - loss: 12.3332 - accuracy: 0.100 - ETA: 1s - loss: 12.3281 - accuracy: 0.100 - ETA: 1s - loss: 12.3309 - accuracy: 0.100 - ETA: 0s - loss: 12.3312 - accuracy: 0.100 - ETA: 0s - loss: 12.3368 - accuracy: 0.100 - ETA: 0s - loss: 12.3392 - accuracy: 0.100 - ETA: 0s - loss: 12.3377 - accuracy: 0.100 - ETA: 0s - loss: 12.3376 - accuracy: 0.100 - ETA: 0s - loss: 12.3445 - accuracy: 0.099 - ETA: 0s - loss: 12.3507 - accuracy: 0.100 - ETA: 0s - loss: 12.3546 - accuracy: 0.100 - ETA: 0s - loss: 12.3537 - accuracy: 0.100 - ETA: 0s - loss: 12.3590 - accuracy: 0.099 - ETA: 0s - loss: 12.3603 - accuracy: 0.100 - ETA: 0s - loss: 12.3682 - accuracy: 0.100 - ETA: 0s - loss: 12.3691 - accuracy: 0.100 - ETA: 0s - loss: 12.3706 - accuracy: 0.100 - ETA: 0s - loss: 12.3763 - accuracy: 0.100 - ETA: 0s - loss: 12.3830 - accuracy: 0.099 - ETA: 0s - loss: 12.3899 - accuracy: 0.099 - ETA: 0s - loss: 12.3864 - accuracy: 0.099 - ETA: 0s - loss: 12.3879 - accuracy: 0.099 - ETA: 0s - loss: 12.3916 - accuracy: 0.099 - ETA: 0s - loss: 12.3981 - accuracy: 0.099 - 4s 2ms/step - loss: 12.3962 - accuracy: 0.0992\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 12.6835 - accuracy: 0.093 - ETA: 3s - loss: 12.2544 - accuracy: 0.115 - ETA: 3s - loss: 12.1597 - accuracy: 0.125 - ETA: 3s - loss: 12.1340 - accuracy: 0.118 - ETA: 3s - loss: 12.2457 - accuracy: 0.114 - ETA: 3s - loss: 12.2850 - accuracy: 0.108 - ETA: 3s - loss: 12.3237 - accuracy: 0.107 - ETA: 3s - loss: 12.3670 - accuracy: 0.104 - ETA: 3s - loss: 12.4594 - accuracy: 0.103 - ETA: 3s - loss: 12.4567 - accuracy: 0.102 - ETA: 2s - loss: 12.4748 - accuracy: 0.102 - ETA: 2s - loss: 12.4928 - accuracy: 0.101 - ETA: 2s - loss: 12.4823 - accuracy: 0.102 - ETA: 2s - loss: 12.5039 - accuracy: 0.101 - ETA: 2s - loss: 12.5210 - accuracy: 0.100 - ETA: 2s - loss: 12.5443 - accuracy: 0.099 - ETA: 2s - loss: 12.5297 - accuracy: 0.099 - ETA: 2s - loss: 12.5358 - accuracy: 0.099 - ETA: 2s - loss: 12.5389 - accuracy: 0.100 - ETA: 2s - loss: 12.5366 - accuracy: 0.100 - ETA: 2s - loss: 12.5319 - accuracy: 0.100 - ETA: 2s - loss: 12.5266 - accuracy: 0.100 - ETA: 2s - loss: 12.5196 - accuracy: 0.100 - ETA: 1s - loss: 12.5204 - accuracy: 0.099 - ETA: 1s - loss: 12.5013 - accuracy: 0.099 - ETA: 1s - loss: 12.4947 - accuracy: 0.100 - ETA: 1s - loss: 12.4953 - accuracy: 0.100 - ETA: 1s - loss: 12.5137 - accuracy: 0.099 - ETA: 1s - loss: 12.5208 - accuracy: 0.099 - ETA: 1s - loss: 12.5235 - accuracy: 0.099 - ETA: 1s - loss: 12.5304 - accuracy: 0.099 - ETA: 1s - loss: 12.5281 - accuracy: 0.099 - ETA: 1s - loss: 12.5190 - accuracy: 0.099 - ETA: 1s - loss: 12.5189 - accuracy: 0.099 - ETA: 1s - loss: 12.5290 - accuracy: 0.099 - ETA: 1s - loss: 12.5337 - accuracy: 0.099 - ETA: 1s - loss: 12.5373 - accuracy: 0.098 - ETA: 1s - loss: 12.5295 - accuracy: 0.098 - ETA: 1s - loss: 12.5250 - accuracy: 0.097 - ETA: 1s - loss: 12.5201 - accuracy: 0.097 - ETA: 1s - loss: 12.5189 - accuracy: 0.097 - ETA: 1s - loss: 12.5168 - accuracy: 0.097 - ETA: 0s - loss: 12.5243 - accuracy: 0.097 - ETA: 0s - loss: 12.5132 - accuracy: 0.097 - ETA: 0s - loss: 12.5172 - accuracy: 0.097 - ETA: 0s - loss: 12.5175 - accuracy: 0.097 - ETA: 0s - loss: 12.5152 - accuracy: 0.097 - ETA: 0s - loss: 12.5127 - accuracy: 0.098 - ETA: 0s - loss: 12.5129 - accuracy: 0.098 - ETA: 0s - loss: 12.5142 - accuracy: 0.098 - ETA: 0s - loss: 12.5221 - accuracy: 0.098 - ETA: 0s - loss: 12.5220 - accuracy: 0.098 - ETA: 0s - loss: 12.5216 - accuracy: 0.098 - ETA: 0s - loss: 12.5288 - accuracy: 0.098 - ETA: 0s - loss: 12.5308 - accuracy: 0.098 - ETA: 0s - loss: 12.5253 - accuracy: 0.098 - ETA: 0s - loss: 12.5335 - accuracy: 0.098 - ETA: 0s - loss: 12.5369 - accuracy: 0.098 - ETA: 0s - loss: 12.5351 - accuracy: 0.098 - ETA: 0s - loss: 12.5344 - accuracy: 0.098 - ETA: 0s - loss: 12.5400 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 14.1946 - accuracy: 0.031 - ETA: 6s - loss: 12.8994 - accuracy: 0.078 - ETA: 5s - loss: 12.7872 - accuracy: 0.076 - ETA: 5s - loss: 12.7310 - accuracy: 0.086 - ETA: 4s - loss: 12.6644 - accuracy: 0.093 - ETA: 4s - loss: 12.6027 - accuracy: 0.094 - ETA: 3s - loss: 12.5492 - accuracy: 0.095 - ETA: 3s - loss: 12.5568 - accuracy: 0.094 - ETA: 3s - loss: 12.6066 - accuracy: 0.094 - ETA: 3s - loss: 12.5930 - accuracy: 0.096 - ETA: 3s - loss: 12.5706 - accuracy: 0.096 - ETA: 3s - loss: 12.5720 - accuracy: 0.096 - ETA: 3s - loss: 12.5762 - accuracy: 0.095 - ETA: 3s - loss: 12.5688 - accuracy: 0.096 - ETA: 3s - loss: 12.5537 - accuracy: 0.096 - ETA: 2s - loss: 12.5569 - accuracy: 0.095 - ETA: 2s - loss: 12.5818 - accuracy: 0.095 - ETA: 2s - loss: 12.5845 - accuracy: 0.094 - ETA: 2s - loss: 12.5770 - accuracy: 0.095 - ETA: 2s - loss: 12.5506 - accuracy: 0.095 - ETA: 2s - loss: 12.5605 - accuracy: 0.096 - ETA: 2s - loss: 12.5452 - accuracy: 0.096 - ETA: 2s - loss: 12.5389 - accuracy: 0.097 - ETA: 2s - loss: 12.5580 - accuracy: 0.096 - ETA: 2s - loss: 12.5564 - accuracy: 0.097 - ETA: 2s - loss: 12.5521 - accuracy: 0.097 - ETA: 2s - loss: 12.5413 - accuracy: 0.097 - ETA: 2s - loss: 12.5465 - accuracy: 0.096 - ETA: 2s - loss: 12.5396 - accuracy: 0.096 - ETA: 1s - loss: 12.5319 - accuracy: 0.096 - ETA: 1s - loss: 12.5231 - accuracy: 0.096 - ETA: 1s - loss: 12.5344 - accuracy: 0.096 - ETA: 1s - loss: 12.5553 - accuracy: 0.096 - ETA: 1s - loss: 12.5589 - accuracy: 0.096 - ETA: 1s - loss: 12.5587 - accuracy: 0.096 - ETA: 1s - loss: 12.5632 - accuracy: 0.096 - ETA: 1s - loss: 12.5681 - accuracy: 0.096 - ETA: 1s - loss: 12.5688 - accuracy: 0.096 - ETA: 1s - loss: 12.5736 - accuracy: 0.096 - ETA: 1s - loss: 12.5648 - accuracy: 0.096 - ETA: 1s - loss: 12.5644 - accuracy: 0.096 - ETA: 1s - loss: 12.5726 - accuracy: 0.096 - ETA: 1s - loss: 12.5691 - accuracy: 0.096 - ETA: 1s - loss: 12.5716 - accuracy: 0.096 - ETA: 1s - loss: 12.5669 - accuracy: 0.097 - ETA: 1s - loss: 12.5689 - accuracy: 0.096 - ETA: 0s - loss: 12.5749 - accuracy: 0.096 - ETA: 0s - loss: 12.5695 - accuracy: 0.096 - ETA: 0s - loss: 12.5647 - accuracy: 0.097 - ETA: 0s - loss: 12.5601 - accuracy: 0.096 - ETA: 0s - loss: 12.5536 - accuracy: 0.097 - ETA: 0s - loss: 12.5511 - accuracy: 0.096 - ETA: 0s - loss: 12.5607 - accuracy: 0.096 - ETA: 0s - loss: 12.5572 - accuracy: 0.097 - ETA: 0s - loss: 12.5468 - accuracy: 0.097 - ETA: 0s - loss: 12.5475 - accuracy: 0.097 - ETA: 0s - loss: 12.5406 - accuracy: 0.097 - ETA: 0s - loss: 12.5440 - accuracy: 0.097 - ETA: 0s - loss: 12.5418 - accuracy: 0.097 - ETA: 0s - loss: 12.5444 - accuracy: 0.097 - ETA: 0s - loss: 12.5441 - accuracy: 0.097 - ETA: 0s - loss: 12.5425 - accuracy: 0.097 - ETA: 0s - loss: 12.5408 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 12.1798 - accuracy: 0.156 - ETA: 2s - loss: 12.6403 - accuracy: 0.106 - ETA: 2s - loss: 12.4665 - accuracy: 0.101 - ETA: 2s - loss: 12.5350 - accuracy: 0.102 - ETA: 2s - loss: 12.6418 - accuracy: 0.101 - ETA: 2s - loss: 12.6418 - accuracy: 0.099 - ETA: 2s - loss: 12.6366 - accuracy: 0.100 - ETA: 2s - loss: 12.6496 - accuracy: 0.099 - ETA: 2s - loss: 12.6187 - accuracy: 0.099 - ETA: 2s - loss: 12.5932 - accuracy: 0.099 - ETA: 2s - loss: 12.6120 - accuracy: 0.098 - ETA: 2s - loss: 12.6060 - accuracy: 0.098 - ETA: 2s - loss: 12.5942 - accuracy: 0.098 - ETA: 2s - loss: 12.5848 - accuracy: 0.098 - ETA: 2s - loss: 12.5926 - accuracy: 0.098 - ETA: 2s - loss: 12.5843 - accuracy: 0.098 - ETA: 1s - loss: 12.5919 - accuracy: 0.097 - ETA: 1s - loss: 12.5889 - accuracy: 0.098 - ETA: 1s - loss: 12.5973 - accuracy: 0.097 - ETA: 1s - loss: 12.5894 - accuracy: 0.098 - ETA: 1s - loss: 12.5908 - accuracy: 0.098 - ETA: 1s - loss: 12.5874 - accuracy: 0.098 - ETA: 1s - loss: 12.5743 - accuracy: 0.098 - ETA: 1s - loss: 12.5891 - accuracy: 0.097 - ETA: 1s - loss: 12.5963 - accuracy: 0.097 - ETA: 1s - loss: 12.5955 - accuracy: 0.097 - ETA: 1s - loss: 12.5874 - accuracy: 0.098 - ETA: 1s - loss: 12.5755 - accuracy: 0.098 - ETA: 1s - loss: 12.5719 - accuracy: 0.097 - ETA: 1s - loss: 12.5643 - accuracy: 0.098 - ETA: 1s - loss: 12.5602 - accuracy: 0.098 - ETA: 1s - loss: 12.5599 - accuracy: 0.098 - ETA: 1s - loss: 12.5563 - accuracy: 0.098 - ETA: 1s - loss: 12.5570 - accuracy: 0.098 - ETA: 1s - loss: 12.5520 - accuracy: 0.098 - ETA: 1s - loss: 12.5496 - accuracy: 0.097 - ETA: 1s - loss: 12.5279 - accuracy: 0.097 - ETA: 0s - loss: 12.5274 - accuracy: 0.097 - ETA: 0s - loss: 12.5338 - accuracy: 0.097 - ETA: 0s - loss: 12.5323 - accuracy: 0.097 - ETA: 0s - loss: 12.5331 - accuracy: 0.097 - ETA: 0s - loss: 12.5278 - accuracy: 0.097 - ETA: 0s - loss: 12.5324 - accuracy: 0.097 - ETA: 0s - loss: 12.5299 - accuracy: 0.097 - ETA: 0s - loss: 12.5248 - accuracy: 0.097 - ETA: 0s - loss: 12.5307 - accuracy: 0.097 - ETA: 0s - loss: 12.5270 - accuracy: 0.097 - ETA: 0s - loss: 12.5393 - accuracy: 0.096 - ETA: 0s - loss: 12.5410 - accuracy: 0.097 - ETA: 0s - loss: 12.5379 - accuracy: 0.097 - ETA: 0s - loss: 12.5345 - accuracy: 0.097 - ETA: 0s - loss: 12.5381 - accuracy: 0.097 - ETA: 0s - loss: 12.5356 - accuracy: 0.097 - ETA: 0s - loss: 12.5347 - accuracy: 0.097 - ETA: 0s - loss: 12.5424 - accuracy: 0.097 - ETA: 0s - loss: 12.5441 - accuracy: 0.097 - ETA: 0s - loss: 12.5423 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 13.6909 - accuracy: 0.0000e+0 - ETA: 3s - loss: 12.3645 - accuracy: 0.1083    - ETA: 3s - loss: 12.4143 - accuracy: 0.103 - ETA: 3s - loss: 12.5250 - accuracy: 0.098 - ETA: 3s - loss: 12.5662 - accuracy: 0.097 - ETA: 3s - loss: 12.5720 - accuracy: 0.097 - ETA: 3s - loss: 12.5116 - accuracy: 0.100 - ETA: 2s - loss: 12.4561 - accuracy: 0.100 - ETA: 2s - loss: 12.4566 - accuracy: 0.100 - ETA: 2s - loss: 12.4552 - accuracy: 0.100 - ETA: 2s - loss: 12.4768 - accuracy: 0.099 - ETA: 2s - loss: 12.4957 - accuracy: 0.100 - ETA: 2s - loss: 12.5068 - accuracy: 0.100 - ETA: 2s - loss: 12.5341 - accuracy: 0.098 - ETA: 2s - loss: 12.5433 - accuracy: 0.098 - ETA: 2s - loss: 12.5453 - accuracy: 0.098 - ETA: 2s - loss: 12.5431 - accuracy: 0.098 - ETA: 2s - loss: 12.5230 - accuracy: 0.098 - ETA: 2s - loss: 12.5144 - accuracy: 0.098 - ETA: 2s - loss: 12.5108 - accuracy: 0.097 - ETA: 2s - loss: 12.5140 - accuracy: 0.098 - ETA: 1s - loss: 12.5102 - accuracy: 0.097 - ETA: 1s - loss: 12.5136 - accuracy: 0.097 - ETA: 1s - loss: 12.5151 - accuracy: 0.097 - ETA: 1s - loss: 12.5069 - accuracy: 0.098 - ETA: 1s - loss: 12.5066 - accuracy: 0.098 - ETA: 1s - loss: 12.5010 - accuracy: 0.099 - ETA: 1s - loss: 12.5042 - accuracy: 0.098 - ETA: 1s - loss: 12.5059 - accuracy: 0.098 - ETA: 1s - loss: 12.5112 - accuracy: 0.098 - ETA: 1s - loss: 12.4928 - accuracy: 0.098 - ETA: 1s - loss: 12.4911 - accuracy: 0.098 - ETA: 1s - loss: 12.4940 - accuracy: 0.098 - ETA: 1s - loss: 12.4928 - accuracy: 0.098 - ETA: 1s - loss: 12.5015 - accuracy: 0.098 - ETA: 1s - loss: 12.5040 - accuracy: 0.097 - ETA: 1s - loss: 12.4954 - accuracy: 0.098 - ETA: 1s - loss: 12.4967 - accuracy: 0.098 - ETA: 1s - loss: 12.5037 - accuracy: 0.097 - ETA: 1s - loss: 12.5101 - accuracy: 0.097 - ETA: 1s - loss: 12.5092 - accuracy: 0.098 - ETA: 0s - loss: 12.5066 - accuracy: 0.097 - ETA: 0s - loss: 12.5084 - accuracy: 0.097 - ETA: 0s - loss: 12.5104 - accuracy: 0.097 - ETA: 0s - loss: 12.5062 - accuracy: 0.098 - ETA: 0s - loss: 12.5090 - accuracy: 0.098 - ETA: 0s - loss: 12.5169 - accuracy: 0.097 - ETA: 0s - loss: 12.5227 - accuracy: 0.097 - ETA: 0s - loss: 12.5323 - accuracy: 0.097 - ETA: 0s - loss: 12.5275 - accuracy: 0.097 - ETA: 0s - loss: 12.5281 - accuracy: 0.098 - ETA: 0s - loss: 12.5293 - accuracy: 0.098 - ETA: 0s - loss: 12.5271 - accuracy: 0.098 - ETA: 0s - loss: 12.5276 - accuracy: 0.097 - ETA: 0s - loss: 12.5330 - accuracy: 0.097 - ETA: 0s - loss: 12.5333 - accuracy: 0.097 - ETA: 0s - loss: 12.5329 - accuracy: 0.097 - ETA: 0s - loss: 12.5369 - accuracy: 0.097 - ETA: 0s - loss: 12.5461 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 9.1577 - accuracy: 0.06 - ETA: 2s - loss: 12.6547 - accuracy: 0.076 - ETA: 2s - loss: 12.5813 - accuracy: 0.082 - ETA: 2s - loss: 12.6736 - accuracy: 0.080 - ETA: 2s - loss: 12.6611 - accuracy: 0.085 - ETA: 2s - loss: 12.6344 - accuracy: 0.091 - ETA: 2s - loss: 12.5828 - accuracy: 0.094 - ETA: 2s - loss: 12.5907 - accuracy: 0.095 - ETA: 2s - loss: 12.5687 - accuracy: 0.096 - ETA: 2s - loss: 12.5855 - accuracy: 0.097 - ETA: 2s - loss: 12.6124 - accuracy: 0.096 - ETA: 2s - loss: 12.5842 - accuracy: 0.097 - ETA: 2s - loss: 12.5454 - accuracy: 0.099 - ETA: 2s - loss: 12.5156 - accuracy: 0.099 - ETA: 2s - loss: 12.5094 - accuracy: 0.098 - ETA: 2s - loss: 12.4899 - accuracy: 0.099 - ETA: 2s - loss: 12.4934 - accuracy: 0.099 - ETA: 2s - loss: 12.5206 - accuracy: 0.098 - ETA: 1s - loss: 12.5106 - accuracy: 0.098 - ETA: 1s - loss: 12.5148 - accuracy: 0.098 - ETA: 1s - loss: 12.5280 - accuracy: 0.098 - ETA: 1s - loss: 12.5332 - accuracy: 0.097 - ETA: 1s - loss: 12.5489 - accuracy: 0.097 - ETA: 1s - loss: 12.5580 - accuracy: 0.097 - ETA: 1s - loss: 12.5543 - accuracy: 0.097 - ETA: 1s - loss: 12.5468 - accuracy: 0.098 - ETA: 1s - loss: 12.5455 - accuracy: 0.097 - ETA: 1s - loss: 12.5318 - accuracy: 0.097 - ETA: 1s - loss: 12.5253 - accuracy: 0.097 - ETA: 1s - loss: 12.5329 - accuracy: 0.098 - ETA: 1s - loss: 12.5290 - accuracy: 0.098 - ETA: 1s - loss: 12.5249 - accuracy: 0.098 - ETA: 1s - loss: 12.5301 - accuracy: 0.098 - ETA: 1s - loss: 12.5353 - accuracy: 0.098 - ETA: 1s - loss: 12.5352 - accuracy: 0.098 - ETA: 1s - loss: 12.5374 - accuracy: 0.097 - ETA: 1s - loss: 12.5324 - accuracy: 0.098 - ETA: 0s - loss: 12.5339 - accuracy: 0.098 - ETA: 0s - loss: 12.5368 - accuracy: 0.098 - ETA: 0s - loss: 12.5312 - accuracy: 0.098 - ETA: 0s - loss: 12.5307 - accuracy: 0.098 - ETA: 0s - loss: 12.5369 - accuracy: 0.098 - ETA: 0s - loss: 12.5360 - accuracy: 0.098 - ETA: 0s - loss: 12.5239 - accuracy: 0.098 - ETA: 0s - loss: 12.5256 - accuracy: 0.098 - ETA: 0s - loss: 12.5270 - accuracy: 0.098 - ETA: 0s - loss: 12.5308 - accuracy: 0.098 - ETA: 0s - loss: 12.5254 - accuracy: 0.098 - ETA: 0s - loss: 12.5244 - accuracy: 0.098 - ETA: 0s - loss: 12.5136 - accuracy: 0.098 - ETA: 0s - loss: 12.5186 - accuracy: 0.098 - ETA: 0s - loss: 12.5223 - accuracy: 0.098 - ETA: 0s - loss: 12.5234 - accuracy: 0.098 - ETA: 0s - loss: 12.5299 - accuracy: 0.097 - ETA: 0s - loss: 12.5311 - accuracy: 0.097 - ETA: 0s - loss: 12.5323 - accuracy: 0.097 - ETA: 0s - loss: 12.5347 - accuracy: 0.097 - ETA: 0s - loss: 12.5360 - accuracy: 0.097 - ETA: 0s - loss: 12.5352 - accuracy: 0.097 - ETA: 0s - loss: 12.5402 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 13.1872 - accuracy: 0.125 - ETA: 2s - loss: 12.5072 - accuracy: 0.103 - ETA: 2s - loss: 12.4009 - accuracy: 0.103 - ETA: 2s - loss: 12.4876 - accuracy: 0.097 - ETA: 2s - loss: 12.5673 - accuracy: 0.095 - ETA: 1s - loss: 12.5369 - accuracy: 0.097 - ETA: 2s - loss: 12.6009 - accuracy: 0.096 - ETA: 2s - loss: 12.5835 - accuracy: 0.097 - ETA: 2s - loss: 12.5857 - accuracy: 0.096 - ETA: 2s - loss: 12.5614 - accuracy: 0.096 - ETA: 2s - loss: 12.5625 - accuracy: 0.095 - ETA: 2s - loss: 12.5507 - accuracy: 0.093 - ETA: 2s - loss: 12.5451 - accuracy: 0.092 - ETA: 2s - loss: 12.5310 - accuracy: 0.093 - ETA: 2s - loss: 12.5208 - accuracy: 0.093 - ETA: 2s - loss: 12.5344 - accuracy: 0.094 - ETA: 2s - loss: 12.5490 - accuracy: 0.093 - ETA: 1s - loss: 12.5508 - accuracy: 0.094 - ETA: 1s - loss: 12.5528 - accuracy: 0.094 - ETA: 1s - loss: 12.5655 - accuracy: 0.095 - ETA: 1s - loss: 12.5602 - accuracy: 0.095 - ETA: 1s - loss: 12.5484 - accuracy: 0.095 - ETA: 1s - loss: 12.5507 - accuracy: 0.096 - ETA: 1s - loss: 12.5439 - accuracy: 0.095 - ETA: 1s - loss: 12.5462 - accuracy: 0.095 - ETA: 1s - loss: 12.5592 - accuracy: 0.095 - ETA: 1s - loss: 12.5601 - accuracy: 0.095 - ETA: 1s - loss: 12.5582 - accuracy: 0.096 - ETA: 1s - loss: 12.5631 - accuracy: 0.096 - ETA: 1s - loss: 12.5639 - accuracy: 0.096 - ETA: 1s - loss: 12.5557 - accuracy: 0.096 - ETA: 1s - loss: 12.5666 - accuracy: 0.096 - ETA: 1s - loss: 12.5705 - accuracy: 0.095 - ETA: 1s - loss: 12.5641 - accuracy: 0.095 - ETA: 1s - loss: 12.5536 - accuracy: 0.096 - ETA: 1s - loss: 12.5474 - accuracy: 0.096 - ETA: 1s - loss: 12.5504 - accuracy: 0.096 - ETA: 0s - loss: 12.5449 - accuracy: 0.097 - ETA: 0s - loss: 12.5509 - accuracy: 0.096 - ETA: 0s - loss: 12.5494 - accuracy: 0.096 - ETA: 0s - loss: 12.5518 - accuracy: 0.097 - ETA: 0s - loss: 12.5501 - accuracy: 0.096 - ETA: 0s - loss: 12.5519 - accuracy: 0.097 - ETA: 0s - loss: 12.5546 - accuracy: 0.097 - ETA: 0s - loss: 12.5524 - accuracy: 0.096 - ETA: 0s - loss: 12.5536 - accuracy: 0.096 - ETA: 0s - loss: 12.5483 - accuracy: 0.097 - ETA: 0s - loss: 12.5552 - accuracy: 0.097 - ETA: 0s - loss: 12.5514 - accuracy: 0.097 - ETA: 0s - loss: 12.5465 - accuracy: 0.097 - ETA: 0s - loss: 12.5482 - accuracy: 0.097 - ETA: 0s - loss: 12.5461 - accuracy: 0.097 - ETA: 0s - loss: 12.5441 - accuracy: 0.097 - ETA: 0s - loss: 12.5392 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 12.1798 - accuracy: 0.187 - ETA: 2s - loss: 12.3701 - accuracy: 0.111 - ETA: 2s - loss: 12.4259 - accuracy: 0.099 - ETA: 2s - loss: 12.5117 - accuracy: 0.098 - ETA: 2s - loss: 12.5722 - accuracy: 0.097 - ETA: 1s - loss: 12.5552 - accuracy: 0.099 - ETA: 1s - loss: 12.5169 - accuracy: 0.097 - ETA: 1s - loss: 12.5377 - accuracy: 0.098 - ETA: 1s - loss: 12.5437 - accuracy: 0.097 - ETA: 1s - loss: 12.5337 - accuracy: 0.098 - ETA: 1s - loss: 12.5288 - accuracy: 0.098 - ETA: 1s - loss: 12.5544 - accuracy: 0.097 - ETA: 1s - loss: 12.5314 - accuracy: 0.098 - ETA: 1s - loss: 12.5210 - accuracy: 0.098 - ETA: 1s - loss: 12.5408 - accuracy: 0.097 - ETA: 1s - loss: 12.5401 - accuracy: 0.097 - ETA: 1s - loss: 12.5343 - accuracy: 0.097 - ETA: 1s - loss: 12.5348 - accuracy: 0.096 - ETA: 1s - loss: 12.5344 - accuracy: 0.097 - ETA: 1s - loss: 12.5394 - accuracy: 0.097 - ETA: 1s - loss: 12.5292 - accuracy: 0.097 - ETA: 1s - loss: 12.5219 - accuracy: 0.096 - ETA: 1s - loss: 12.5198 - accuracy: 0.097 - ETA: 1s - loss: 12.5135 - accuracy: 0.098 - ETA: 1s - loss: 12.5241 - accuracy: 0.097 - ETA: 0s - loss: 12.5292 - accuracy: 0.097 - ETA: 0s - loss: 12.5491 - accuracy: 0.097 - ETA: 0s - loss: 12.5290 - accuracy: 0.097 - ETA: 0s - loss: 12.5422 - accuracy: 0.097 - ETA: 0s - loss: 12.5514 - accuracy: 0.097 - ETA: 0s - loss: 12.5464 - accuracy: 0.097 - ETA: 0s - loss: 12.5546 - accuracy: 0.097 - ETA: 0s - loss: 12.5465 - accuracy: 0.097 - ETA: 0s - loss: 12.5499 - accuracy: 0.097 - ETA: 0s - loss: 12.5508 - accuracy: 0.097 - ETA: 0s - loss: 12.5434 - accuracy: 0.097 - ETA: 0s - loss: 12.5448 - accuracy: 0.097 - ETA: 0s - loss: 12.5517 - accuracy: 0.097 - ETA: 0s - loss: 12.5470 - accuracy: 0.097 - ETA: 0s - loss: 12.5448 - accuracy: 0.097 - ETA: 0s - loss: 12.5379 - accuracy: 0.097 - ETA: 0s - loss: 12.5378 - accuracy: 0.097 - ETA: 0s - loss: 12.5406 - accuracy: 0.097 - ETA: 0s - loss: 12.5446 - accuracy: 0.097 - ETA: 0s - loss: 12.5397 - accuracy: 0.097 - 2s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 12.1798 - accuracy: 0.093 - ETA: 2s - loss: 12.5690 - accuracy: 0.097 - ETA: 2s - loss: 12.5330 - accuracy: 0.099 - ETA: 2s - loss: 12.5643 - accuracy: 0.095 - ETA: 1s - loss: 12.5966 - accuracy: 0.097 - ETA: 1s - loss: 12.5860 - accuracy: 0.096 - ETA: 1s - loss: 12.5668 - accuracy: 0.097 - ETA: 1s - loss: 12.5350 - accuracy: 0.098 - ETA: 1s - loss: 12.5068 - accuracy: 0.098 - ETA: 1s - loss: 12.5043 - accuracy: 0.098 - ETA: 1s - loss: 12.4965 - accuracy: 0.098 - ETA: 1s - loss: 12.5202 - accuracy: 0.097 - ETA: 1s - loss: 12.5143 - accuracy: 0.097 - ETA: 1s - loss: 12.5002 - accuracy: 0.098 - ETA: 1s - loss: 12.5032 - accuracy: 0.097 - ETA: 1s - loss: 12.5012 - accuracy: 0.098 - ETA: 1s - loss: 12.5141 - accuracy: 0.097 - ETA: 1s - loss: 12.5147 - accuracy: 0.097 - ETA: 1s - loss: 12.5373 - accuracy: 0.097 - ETA: 1s - loss: 12.5290 - accuracy: 0.097 - ETA: 1s - loss: 12.5375 - accuracy: 0.098 - ETA: 1s - loss: 12.5417 - accuracy: 0.097 - ETA: 1s - loss: 12.5614 - accuracy: 0.096 - ETA: 1s - loss: 12.5568 - accuracy: 0.097 - ETA: 1s - loss: 12.5592 - accuracy: 0.097 - ETA: 1s - loss: 12.5442 - accuracy: 0.097 - ETA: 0s - loss: 12.5407 - accuracy: 0.097 - ETA: 0s - loss: 12.5477 - accuracy: 0.097 - ETA: 0s - loss: 12.5558 - accuracy: 0.097 - ETA: 0s - loss: 12.5537 - accuracy: 0.097 - ETA: 0s - loss: 12.5496 - accuracy: 0.097 - ETA: 0s - loss: 12.5559 - accuracy: 0.097 - ETA: 0s - loss: 12.5481 - accuracy: 0.097 - ETA: 0s - loss: 12.5452 - accuracy: 0.097 - ETA: 0s - loss: 12.5359 - accuracy: 0.097 - ETA: 0s - loss: 12.5425 - accuracy: 0.096 - ETA: 0s - loss: 12.5375 - accuracy: 0.097 - ETA: 0s - loss: 12.5361 - accuracy: 0.097 - ETA: 0s - loss: 12.5324 - accuracy: 0.097 - ETA: 0s - loss: 12.5424 - accuracy: 0.096 - ETA: 0s - loss: 12.5391 - accuracy: 0.096 - ETA: 0s - loss: 12.5465 - accuracy: 0.096 - ETA: 0s - loss: 12.5508 - accuracy: 0.096 - ETA: 0s - loss: 12.5448 - accuracy: 0.097 - ETA: 0s - loss: 12.5430 - accuracy: 0.097 - ETA: 0s - loss: 12.5400 - accuracy: 0.097 - ETA: 0s - loss: 12.5470 - accuracy: 0.097 - ETA: 0s - loss: 12.5424 - accuracy: 0.097 - ETA: 0s - loss: 12.5414 - accuracy: 0.097 - 2s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 13.6909 - accuracy: 0.062 - ETA: 2s - loss: 12.5636 - accuracy: 0.097 - ETA: 2s - loss: 12.5591 - accuracy: 0.092 - ETA: 2s - loss: 12.6347 - accuracy: 0.094 - ETA: 2s - loss: 12.5803 - accuracy: 0.094 - ETA: 2s - loss: 12.5278 - accuracy: 0.096 - ETA: 2s - loss: 12.5169 - accuracy: 0.097 - ETA: 1s - loss: 12.5331 - accuracy: 0.098 - ETA: 1s - loss: 12.5307 - accuracy: 0.097 - ETA: 1s - loss: 12.5687 - accuracy: 0.095 - ETA: 2s - loss: 12.5426 - accuracy: 0.095 - ETA: 2s - loss: 12.5406 - accuracy: 0.096 - ETA: 1s - loss: 12.5255 - accuracy: 0.096 - ETA: 1s - loss: 12.5410 - accuracy: 0.096 - ETA: 1s - loss: 12.5573 - accuracy: 0.095 - ETA: 1s - loss: 12.5578 - accuracy: 0.095 - ETA: 1s - loss: 12.5436 - accuracy: 0.096 - ETA: 1s - loss: 12.5558 - accuracy: 0.095 - ETA: 1s - loss: 12.5563 - accuracy: 0.095 - ETA: 1s - loss: 12.5564 - accuracy: 0.095 - ETA: 1s - loss: 12.5559 - accuracy: 0.095 - ETA: 1s - loss: 12.5634 - accuracy: 0.095 - ETA: 1s - loss: 12.5719 - accuracy: 0.095 - ETA: 1s - loss: 12.5737 - accuracy: 0.095 - ETA: 1s - loss: 12.5770 - accuracy: 0.094 - ETA: 1s - loss: 12.5733 - accuracy: 0.095 - ETA: 1s - loss: 12.5786 - accuracy: 0.095 - ETA: 1s - loss: 12.5735 - accuracy: 0.095 - ETA: 1s - loss: 12.5787 - accuracy: 0.095 - ETA: 1s - loss: 12.5684 - accuracy: 0.096 - ETA: 1s - loss: 12.5625 - accuracy: 0.096 - ETA: 1s - loss: 12.5683 - accuracy: 0.096 - ETA: 1s - loss: 12.5650 - accuracy: 0.096 - ETA: 1s - loss: 12.5677 - accuracy: 0.096 - ETA: 1s - loss: 12.5617 - accuracy: 0.096 - ETA: 1s - loss: 12.5653 - accuracy: 0.096 - ETA: 0s - loss: 12.5636 - accuracy: 0.096 - ETA: 0s - loss: 12.5660 - accuracy: 0.096 - ETA: 0s - loss: 12.5702 - accuracy: 0.096 - ETA: 0s - loss: 12.5685 - accuracy: 0.096 - ETA: 0s - loss: 12.5610 - accuracy: 0.097 - ETA: 0s - loss: 12.5552 - accuracy: 0.096 - ETA: 0s - loss: 12.5659 - accuracy: 0.096 - ETA: 0s - loss: 12.5643 - accuracy: 0.096 - ETA: 0s - loss: 12.5663 - accuracy: 0.096 - ETA: 0s - loss: 12.5716 - accuracy: 0.096 - ETA: 0s - loss: 12.5662 - accuracy: 0.097 - ETA: 0s - loss: 12.5627 - accuracy: 0.096 - ETA: 0s - loss: 12.5607 - accuracy: 0.096 - ETA: 0s - loss: 12.5615 - accuracy: 0.097 - ETA: 0s - loss: 12.5585 - accuracy: 0.097 - ETA: 0s - loss: 12.5540 - accuracy: 0.097 - ETA: 0s - loss: 12.5527 - accuracy: 0.097 - ETA: 0s - loss: 12.5542 - accuracy: 0.096 - ETA: 0s - loss: 12.5480 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 15.2019 - accuracy: 0.0000e+0 - ETA: 2s - loss: 12.3786 - accuracy: 0.1077    - ETA: 2s - loss: 12.5179 - accuracy: 0.107 - ETA: 2s - loss: 12.5631 - accuracy: 0.104 - ETA: 2s - loss: 12.6242 - accuracy: 0.102 - ETA: 2s - loss: 12.6391 - accuracy: 0.101 - ETA: 2s - loss: 12.6065 - accuracy: 0.099 - ETA: 2s - loss: 12.6142 - accuracy: 0.098 - ETA: 2s - loss: 12.6182 - accuracy: 0.097 - ETA: 2s - loss: 12.6313 - accuracy: 0.094 - ETA: 2s - loss: 12.6045 - accuracy: 0.095 - ETA: 2s - loss: 12.6222 - accuracy: 0.094 - ETA: 1s - loss: 12.6249 - accuracy: 0.094 - ETA: 1s - loss: 12.6106 - accuracy: 0.094 - ETA: 1s - loss: 12.6140 - accuracy: 0.094 - ETA: 1s - loss: 12.6166 - accuracy: 0.094 - ETA: 1s - loss: 12.5929 - accuracy: 0.095 - ETA: 1s - loss: 12.5689 - accuracy: 0.095 - ETA: 1s - loss: 12.5746 - accuracy: 0.095 - ETA: 1s - loss: 12.5530 - accuracy: 0.095 - ETA: 1s - loss: 12.5524 - accuracy: 0.095 - ETA: 1s - loss: 12.5595 - accuracy: 0.095 - ETA: 1s - loss: 12.5670 - accuracy: 0.095 - ETA: 1s - loss: 12.5802 - accuracy: 0.094 - ETA: 1s - loss: 12.5588 - accuracy: 0.095 - ETA: 1s - loss: 12.5672 - accuracy: 0.094 - ETA: 1s - loss: 12.5656 - accuracy: 0.094 - ETA: 1s - loss: 12.5525 - accuracy: 0.094 - ETA: 1s - loss: 12.5504 - accuracy: 0.094 - ETA: 1s - loss: 12.5540 - accuracy: 0.094 - ETA: 1s - loss: 12.5411 - accuracy: 0.095 - ETA: 0s - loss: 12.5342 - accuracy: 0.095 - ETA: 0s - loss: 12.5290 - accuracy: 0.095 - ETA: 0s - loss: 12.5337 - accuracy: 0.096 - ETA: 0s - loss: 12.5330 - accuracy: 0.096 - ETA: 0s - loss: 12.5358 - accuracy: 0.096 - ETA: 0s - loss: 12.5369 - accuracy: 0.096 - ETA: 0s - loss: 12.5282 - accuracy: 0.096 - ETA: 0s - loss: 12.5182 - accuracy: 0.096 - ETA: 0s - loss: 12.5245 - accuracy: 0.096 - ETA: 0s - loss: 12.5182 - accuracy: 0.096 - ETA: 0s - loss: 12.5118 - accuracy: 0.097 - ETA: 0s - loss: 12.5215 - accuracy: 0.097 - ETA: 0s - loss: 12.5269 - accuracy: 0.097 - ETA: 0s - loss: 12.5284 - accuracy: 0.097 - ETA: 0s - loss: 12.5318 - accuracy: 0.097 - ETA: 0s - loss: 12.5399 - accuracy: 0.097 - ETA: 0s - loss: 12.5424 - accuracy: 0.097 - ETA: 0s - loss: 12.5464 - accuracy: 0.097 - ETA: 0s - loss: 12.5455 - accuracy: 0.097 - ETA: 0s - loss: 12.5421 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 13.6909 - accuracy: 0.093 - ETA: 2s - loss: 12.2887 - accuracy: 0.103 - ETA: 2s - loss: 12.4806 - accuracy: 0.104 - ETA: 2s - loss: 12.5264 - accuracy: 0.105 - ETA: 2s - loss: 12.5063 - accuracy: 0.104 - ETA: 2s - loss: 12.5100 - accuracy: 0.103 - ETA: 2s - loss: 12.5156 - accuracy: 0.101 - ETA: 2s - loss: 12.5760 - accuracy: 0.102 - ETA: 2s - loss: 12.5202 - accuracy: 0.105 - ETA: 2s - loss: 12.5206 - accuracy: 0.104 - ETA: 2s - loss: 12.5241 - accuracy: 0.102 - ETA: 1s - loss: 12.5371 - accuracy: 0.103 - ETA: 1s - loss: 12.5662 - accuracy: 0.102 - ETA: 1s - loss: 12.5468 - accuracy: 0.101 - ETA: 1s - loss: 12.5492 - accuracy: 0.100 - ETA: 1s - loss: 12.5642 - accuracy: 0.099 - ETA: 1s - loss: 12.5373 - accuracy: 0.099 - ETA: 1s - loss: 12.5418 - accuracy: 0.099 - ETA: 1s - loss: 12.5408 - accuracy: 0.099 - ETA: 1s - loss: 12.5287 - accuracy: 0.099 - ETA: 1s - loss: 12.5372 - accuracy: 0.099 - ETA: 1s - loss: 12.5383 - accuracy: 0.099 - ETA: 1s - loss: 12.5407 - accuracy: 0.099 - ETA: 1s - loss: 12.5580 - accuracy: 0.098 - ETA: 1s - loss: 12.5509 - accuracy: 0.098 - ETA: 1s - loss: 12.5535 - accuracy: 0.099 - ETA: 1s - loss: 12.5459 - accuracy: 0.098 - ETA: 1s - loss: 12.5353 - accuracy: 0.099 - ETA: 1s - loss: 12.5436 - accuracy: 0.099 - ETA: 1s - loss: 12.5433 - accuracy: 0.098 - ETA: 1s - loss: 12.5452 - accuracy: 0.098 - ETA: 0s - loss: 12.5479 - accuracy: 0.097 - ETA: 0s - loss: 12.5460 - accuracy: 0.098 - ETA: 0s - loss: 12.5563 - accuracy: 0.097 - ETA: 0s - loss: 12.5511 - accuracy: 0.098 - ETA: 0s - loss: 12.5553 - accuracy: 0.097 - ETA: 0s - loss: 12.5554 - accuracy: 0.097 - ETA: 0s - loss: 12.5474 - accuracy: 0.097 - ETA: 0s - loss: 12.5539 - accuracy: 0.097 - ETA: 0s - loss: 12.5548 - accuracy: 0.097 - ETA: 0s - loss: 12.5519 - accuracy: 0.097 - ETA: 0s - loss: 12.5492 - accuracy: 0.097 - ETA: 0s - loss: 12.5572 - accuracy: 0.097 - ETA: 0s - loss: 12.5565 - accuracy: 0.097 - ETA: 0s - loss: 12.5597 - accuracy: 0.097 - ETA: 0s - loss: 12.5489 - accuracy: 0.097 - ETA: 0s - loss: 12.5445 - accuracy: 0.097 - ETA: 0s - loss: 12.5455 - accuracy: 0.097 - ETA: 0s - loss: 12.5445 - accuracy: 0.097 - ETA: 0s - loss: 12.5397 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 11.6761 - accuracy: 0.093 - ETA: 2s - loss: 12.6189 - accuracy: 0.088 - ETA: 2s - loss: 12.5134 - accuracy: 0.094 - ETA: 2s - loss: 12.5302 - accuracy: 0.096 - ETA: 2s - loss: 12.5200 - accuracy: 0.099 - ETA: 2s - loss: 12.4343 - accuracy: 0.100 - ETA: 2s - loss: 12.4960 - accuracy: 0.099 - ETA: 2s - loss: 12.4820 - accuracy: 0.100 - ETA: 2s - loss: 12.4290 - accuracy: 0.102 - ETA: 2s - loss: 12.4439 - accuracy: 0.101 - ETA: 2s - loss: 12.4218 - accuracy: 0.100 - ETA: 2s - loss: 12.4352 - accuracy: 0.100 - ETA: 2s - loss: 12.4415 - accuracy: 0.100 - ETA: 2s - loss: 12.4310 - accuracy: 0.101 - ETA: 2s - loss: 12.4517 - accuracy: 0.100 - ETA: 2s - loss: 12.4665 - accuracy: 0.099 - ETA: 2s - loss: 12.4760 - accuracy: 0.099 - ETA: 2s - loss: 12.4915 - accuracy: 0.099 - ETA: 2s - loss: 12.4891 - accuracy: 0.099 - ETA: 2s - loss: 12.4900 - accuracy: 0.099 - ETA: 2s - loss: 12.4887 - accuracy: 0.099 - ETA: 2s - loss: 12.5074 - accuracy: 0.098 - ETA: 2s - loss: 12.5067 - accuracy: 0.098 - ETA: 1s - loss: 12.4928 - accuracy: 0.098 - ETA: 1s - loss: 12.4905 - accuracy: 0.097 - ETA: 1s - loss: 12.4908 - accuracy: 0.097 - ETA: 1s - loss: 12.4855 - accuracy: 0.097 - ETA: 1s - loss: 12.4921 - accuracy: 0.098 - ETA: 1s - loss: 12.4919 - accuracy: 0.098 - ETA: 1s - loss: 12.5016 - accuracy: 0.097 - ETA: 1s - loss: 12.5010 - accuracy: 0.098 - ETA: 1s - loss: 12.5033 - accuracy: 0.098 - ETA: 1s - loss: 12.5179 - accuracy: 0.098 - ETA: 1s - loss: 12.5122 - accuracy: 0.098 - ETA: 1s - loss: 12.5137 - accuracy: 0.098 - ETA: 1s - loss: 12.5050 - accuracy: 0.098 - ETA: 1s - loss: 12.5182 - accuracy: 0.098 - ETA: 1s - loss: 12.5228 - accuracy: 0.098 - ETA: 1s - loss: 12.5244 - accuracy: 0.098 - ETA: 0s - loss: 12.5120 - accuracy: 0.098 - ETA: 0s - loss: 12.5166 - accuracy: 0.098 - ETA: 0s - loss: 12.5193 - accuracy: 0.098 - ETA: 0s - loss: 12.5174 - accuracy: 0.098 - ETA: 0s - loss: 12.5158 - accuracy: 0.098 - ETA: 0s - loss: 12.5201 - accuracy: 0.098 - ETA: 0s - loss: 12.5220 - accuracy: 0.098 - ETA: 0s - loss: 12.5339 - accuracy: 0.098 - ETA: 0s - loss: 12.5344 - accuracy: 0.098 - ETA: 0s - loss: 12.5382 - accuracy: 0.098 - ETA: 0s - loss: 12.5407 - accuracy: 0.097 - ETA: 0s - loss: 12.5378 - accuracy: 0.097 - ETA: 0s - loss: 12.5407 - accuracy: 0.097 - ETA: 0s - loss: 12.5414 - accuracy: 0.097 - ETA: 0s - loss: 12.5455 - accuracy: 0.097 - ETA: 0s - loss: 12.5450 - accuracy: 0.097 - ETA: 0s - loss: 12.5478 - accuracy: 0.097 - ETA: 0s - loss: 12.5409 - accuracy: 0.097 - ETA: 0s - loss: 12.5448 - accuracy: 0.097 - ETA: 0s - loss: 12.5458 - accuracy: 0.097 - 3s 2ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 11.6761 - accuracy: 0.125 - ETA: 2s - loss: 12.8558 - accuracy: 0.091 - ETA: 2s - loss: 12.6499 - accuracy: 0.094 - ETA: 2s - loss: 12.6157 - accuracy: 0.098 - ETA: 2s - loss: 12.6215 - accuracy: 0.097 - ETA: 2s - loss: 12.5730 - accuracy: 0.096 - ETA: 2s - loss: 12.5750 - accuracy: 0.094 - ETA: 2s - loss: 12.6178 - accuracy: 0.094 - ETA: 2s - loss: 12.6035 - accuracy: 0.094 - ETA: 2s - loss: 12.5882 - accuracy: 0.094 - ETA: 2s - loss: 12.5735 - accuracy: 0.094 - ETA: 2s - loss: 12.6051 - accuracy: 0.094 - ETA: 2s - loss: 12.6102 - accuracy: 0.092 - ETA: 2s - loss: 12.5892 - accuracy: 0.093 - ETA: 2s - loss: 12.5840 - accuracy: 0.094 - ETA: 2s - loss: 12.6052 - accuracy: 0.093 - ETA: 2s - loss: 12.5934 - accuracy: 0.094 - ETA: 2s - loss: 12.5894 - accuracy: 0.094 - ETA: 2s - loss: 12.6096 - accuracy: 0.095 - ETA: 2s - loss: 12.6000 - accuracy: 0.095 - ETA: 2s - loss: 12.5814 - accuracy: 0.094 - ETA: 2s - loss: 12.5820 - accuracy: 0.094 - ETA: 1s - loss: 12.5890 - accuracy: 0.095 - ETA: 1s - loss: 12.5883 - accuracy: 0.095 - ETA: 1s - loss: 12.5723 - accuracy: 0.095 - ETA: 1s - loss: 12.5611 - accuracy: 0.096 - ETA: 1s - loss: 12.5722 - accuracy: 0.096 - ETA: 1s - loss: 12.5811 - accuracy: 0.096 - ETA: 1s - loss: 12.5699 - accuracy: 0.096 - ETA: 1s - loss: 12.5642 - accuracy: 0.096 - ETA: 1s - loss: 12.5594 - accuracy: 0.096 - ETA: 1s - loss: 12.5535 - accuracy: 0.096 - ETA: 1s - loss: 12.5560 - accuracy: 0.097 - ETA: 1s - loss: 12.5670 - accuracy: 0.096 - ETA: 1s - loss: 12.5535 - accuracy: 0.097 - ETA: 1s - loss: 12.5659 - accuracy: 0.096 - ETA: 0s - loss: 12.5574 - accuracy: 0.097 - ETA: 0s - loss: 12.5644 - accuracy: 0.097 - ETA: 0s - loss: 12.5501 - accuracy: 0.097 - ETA: 0s - loss: 12.5461 - accuracy: 0.097 - ETA: 0s - loss: 12.5411 - accuracy: 0.097 - ETA: 0s - loss: 12.5386 - accuracy: 0.098 - ETA: 0s - loss: 12.5366 - accuracy: 0.098 - ETA: 0s - loss: 12.5384 - accuracy: 0.097 - ETA: 0s - loss: 12.5381 - accuracy: 0.097 - ETA: 0s - loss: 12.5396 - accuracy: 0.097 - ETA: 0s - loss: 12.5424 - accuracy: 0.097 - ETA: 0s - loss: 12.5472 - accuracy: 0.097 - ETA: 0s - loss: 12.5377 - accuracy: 0.097 - ETA: 0s - loss: 12.5430 - accuracy: 0.097 - ETA: 0s - loss: 12.5393 - accuracy: 0.097 - ETA: 0s - loss: 12.5381 - accuracy: 0.097 - ETA: 0s - loss: 12.5436 - accuracy: 0.097 - ETA: 0s - loss: 12.5430 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 11.1724 - accuracy: 0.062 - ETA: 2s - loss: 12.1168 - accuracy: 0.109 - ETA: 2s - loss: 12.4218 - accuracy: 0.097 - ETA: 2s - loss: 12.4751 - accuracy: 0.099 - ETA: 2s - loss: 12.4495 - accuracy: 0.100 - ETA: 2s - loss: 12.5199 - accuracy: 0.100 - ETA: 2s - loss: 12.5048 - accuracy: 0.101 - ETA: 2s - loss: 12.5089 - accuracy: 0.100 - ETA: 2s - loss: 12.5632 - accuracy: 0.100 - ETA: 1s - loss: 12.5953 - accuracy: 0.100 - ETA: 1s - loss: 12.5933 - accuracy: 0.101 - ETA: 1s - loss: 12.5786 - accuracy: 0.101 - ETA: 1s - loss: 12.6033 - accuracy: 0.099 - ETA: 1s - loss: 12.5790 - accuracy: 0.099 - ETA: 1s - loss: 12.5774 - accuracy: 0.100 - ETA: 1s - loss: 12.5787 - accuracy: 0.099 - ETA: 1s - loss: 12.5819 - accuracy: 0.098 - ETA: 1s - loss: 12.5632 - accuracy: 0.098 - ETA: 1s - loss: 12.5504 - accuracy: 0.098 - ETA: 1s - loss: 12.5647 - accuracy: 0.098 - ETA: 1s - loss: 12.5808 - accuracy: 0.097 - ETA: 1s - loss: 12.5907 - accuracy: 0.097 - ETA: 1s - loss: 12.5726 - accuracy: 0.097 - ETA: 1s - loss: 12.5697 - accuracy: 0.097 - ETA: 1s - loss: 12.5642 - accuracy: 0.098 - ETA: 1s - loss: 12.5603 - accuracy: 0.098 - ETA: 1s - loss: 12.5547 - accuracy: 0.098 - ETA: 1s - loss: 12.5411 - accuracy: 0.098 - ETA: 1s - loss: 12.5359 - accuracy: 0.098 - ETA: 0s - loss: 12.5360 - accuracy: 0.098 - ETA: 0s - loss: 12.5280 - accuracy: 0.098 - ETA: 0s - loss: 12.5370 - accuracy: 0.098 - ETA: 0s - loss: 12.5312 - accuracy: 0.098 - ETA: 0s - loss: 12.5266 - accuracy: 0.098 - ETA: 0s - loss: 12.5302 - accuracy: 0.098 - ETA: 0s - loss: 12.5256 - accuracy: 0.098 - ETA: 0s - loss: 12.5281 - accuracy: 0.098 - ETA: 0s - loss: 12.5300 - accuracy: 0.098 - ETA: 0s - loss: 12.5281 - accuracy: 0.098 - ETA: 0s - loss: 12.5251 - accuracy: 0.098 - ETA: 0s - loss: 12.5197 - accuracy: 0.098 - ETA: 0s - loss: 12.5238 - accuracy: 0.097 - ETA: 0s - loss: 12.5213 - accuracy: 0.098 - ETA: 0s - loss: 12.5293 - accuracy: 0.097 - ETA: 0s - loss: 12.5325 - accuracy: 0.097 - ETA: 0s - loss: 12.5287 - accuracy: 0.097 - ETA: 0s - loss: 12.5339 - accuracy: 0.097 - ETA: 0s - loss: 12.5370 - accuracy: 0.097 - ETA: 0s - loss: 12.5430 - accuracy: 0.097 - ETA: 0s - loss: 12.5376 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 12.1798 - accuracy: 0.187 - ETA: 2s - loss: 12.8724 - accuracy: 0.096 - ETA: 2s - loss: 12.7154 - accuracy: 0.094 - ETA: 2s - loss: 12.5845 - accuracy: 0.093 - ETA: 2s - loss: 12.5210 - accuracy: 0.098 - ETA: 2s - loss: 12.5668 - accuracy: 0.096 - ETA: 2s - loss: 12.5797 - accuracy: 0.099 - ETA: 2s - loss: 12.5774 - accuracy: 0.097 - ETA: 2s - loss: 12.5957 - accuracy: 0.097 - ETA: 2s - loss: 12.6111 - accuracy: 0.097 - ETA: 2s - loss: 12.5841 - accuracy: 0.097 - ETA: 1s - loss: 12.5609 - accuracy: 0.099 - ETA: 1s - loss: 12.5542 - accuracy: 0.099 - ETA: 1s - loss: 12.5777 - accuracy: 0.099 - ETA: 1s - loss: 12.5566 - accuracy: 0.099 - ETA: 1s - loss: 12.5415 - accuracy: 0.099 - ETA: 1s - loss: 12.5578 - accuracy: 0.098 - ETA: 1s - loss: 12.5490 - accuracy: 0.098 - ETA: 1s - loss: 12.5456 - accuracy: 0.098 - ETA: 1s - loss: 12.5447 - accuracy: 0.098 - ETA: 1s - loss: 12.5604 - accuracy: 0.098 - ETA: 1s - loss: 12.5558 - accuracy: 0.098 - ETA: 1s - loss: 12.5378 - accuracy: 0.098 - ETA: 1s - loss: 12.5318 - accuracy: 0.098 - ETA: 1s - loss: 12.5380 - accuracy: 0.098 - ETA: 1s - loss: 12.5318 - accuracy: 0.098 - ETA: 1s - loss: 12.5324 - accuracy: 0.097 - ETA: 1s - loss: 12.5394 - accuracy: 0.098 - ETA: 1s - loss: 12.5353 - accuracy: 0.097 - ETA: 1s - loss: 12.5334 - accuracy: 0.097 - ETA: 1s - loss: 12.5460 - accuracy: 0.097 - ETA: 1s - loss: 12.5377 - accuracy: 0.097 - ETA: 0s - loss: 12.5436 - accuracy: 0.097 - ETA: 0s - loss: 12.5401 - accuracy: 0.097 - ETA: 0s - loss: 12.5412 - accuracy: 0.097 - ETA: 0s - loss: 12.5368 - accuracy: 0.097 - ETA: 0s - loss: 12.5361 - accuracy: 0.097 - ETA: 0s - loss: 12.5394 - accuracy: 0.097 - ETA: 0s - loss: 12.5331 - accuracy: 0.097 - ETA: 0s - loss: 12.5291 - accuracy: 0.097 - ETA: 0s - loss: 12.5233 - accuracy: 0.097 - ETA: 0s - loss: 12.5306 - accuracy: 0.097 - ETA: 0s - loss: 12.5342 - accuracy: 0.097 - ETA: 0s - loss: 12.5318 - accuracy: 0.097 - ETA: 0s - loss: 12.5318 - accuracy: 0.097 - ETA: 0s - loss: 12.5342 - accuracy: 0.097 - ETA: 0s - loss: 12.5305 - accuracy: 0.097 - ETA: 0s - loss: 12.5281 - accuracy: 0.097 - ETA: 0s - loss: 12.5339 - accuracy: 0.097 - ETA: 0s - loss: 12.5357 - accuracy: 0.097 - ETA: 0s - loss: 12.5427 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 13.1872 - accuracy: 0.062 - ETA: 2s - loss: 12.4714 - accuracy: 0.093 - ETA: 2s - loss: 12.4022 - accuracy: 0.105 - ETA: 2s - loss: 12.4864 - accuracy: 0.103 - ETA: 2s - loss: 12.6150 - accuracy: 0.101 - ETA: 2s - loss: 12.6015 - accuracy: 0.099 - ETA: 2s - loss: 12.6247 - accuracy: 0.100 - ETA: 2s - loss: 12.6613 - accuracy: 0.098 - ETA: 2s - loss: 12.6506 - accuracy: 0.098 - ETA: 2s - loss: 12.6766 - accuracy: 0.098 - ETA: 2s - loss: 12.6727 - accuracy: 0.096 - ETA: 2s - loss: 12.6333 - accuracy: 0.097 - ETA: 2s - loss: 12.6708 - accuracy: 0.097 - ETA: 2s - loss: 12.6708 - accuracy: 0.097 - ETA: 2s - loss: 12.6228 - accuracy: 0.098 - ETA: 2s - loss: 12.6204 - accuracy: 0.097 - ETA: 1s - loss: 12.6138 - accuracy: 0.098 - ETA: 1s - loss: 12.6071 - accuracy: 0.097 - ETA: 1s - loss: 12.6141 - accuracy: 0.097 - ETA: 1s - loss: 12.6096 - accuracy: 0.098 - ETA: 1s - loss: 12.6113 - accuracy: 0.097 - ETA: 1s - loss: 12.6081 - accuracy: 0.097 - ETA: 1s - loss: 12.6266 - accuracy: 0.097 - ETA: 1s - loss: 12.6245 - accuracy: 0.097 - ETA: 1s - loss: 12.6016 - accuracy: 0.097 - ETA: 1s - loss: 12.5807 - accuracy: 0.097 - ETA: 1s - loss: 12.5879 - accuracy: 0.096 - ETA: 1s - loss: 12.5802 - accuracy: 0.097 - ETA: 1s - loss: 12.5957 - accuracy: 0.096 - ETA: 1s - loss: 12.5924 - accuracy: 0.096 - ETA: 1s - loss: 12.5824 - accuracy: 0.096 - ETA: 1s - loss: 12.5728 - accuracy: 0.097 - ETA: 1s - loss: 12.5747 - accuracy: 0.096 - ETA: 1s - loss: 12.5676 - accuracy: 0.096 - ETA: 1s - loss: 12.5783 - accuracy: 0.096 - ETA: 0s - loss: 12.5655 - accuracy: 0.097 - ETA: 0s - loss: 12.5462 - accuracy: 0.097 - ETA: 0s - loss: 12.5365 - accuracy: 0.097 - ETA: 0s - loss: 12.5377 - accuracy: 0.097 - ETA: 0s - loss: 12.5297 - accuracy: 0.098 - ETA: 0s - loss: 12.5221 - accuracy: 0.097 - ETA: 0s - loss: 12.5245 - accuracy: 0.097 - ETA: 0s - loss: 12.5258 - accuracy: 0.098 - ETA: 0s - loss: 12.5179 - accuracy: 0.098 - ETA: 0s - loss: 12.5188 - accuracy: 0.098 - ETA: 0s - loss: 12.5166 - accuracy: 0.098 - ETA: 0s - loss: 12.5235 - accuracy: 0.097 - ETA: 0s - loss: 12.5226 - accuracy: 0.097 - ETA: 0s - loss: 12.5276 - accuracy: 0.097 - ETA: 0s - loss: 12.5302 - accuracy: 0.097 - ETA: 0s - loss: 12.5329 - accuracy: 0.097 - ETA: 0s - loss: 12.5304 - accuracy: 0.097 - ETA: 0s - loss: 12.5397 - accuracy: 0.097 - ETA: 0s - loss: 12.5462 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 13.6909 - accuracy: 0.125 - ETA: 2s - loss: 12.6577 - accuracy: 0.109 - ETA: 2s - loss: 12.6704 - accuracy: 0.092 - ETA: 2s - loss: 12.6528 - accuracy: 0.094 - ETA: 2s - loss: 12.7266 - accuracy: 0.092 - ETA: 2s - loss: 12.5822 - accuracy: 0.095 - ETA: 2s - loss: 12.5878 - accuracy: 0.094 - ETA: 2s - loss: 12.5851 - accuracy: 0.093 - ETA: 2s - loss: 12.5576 - accuracy: 0.094 - ETA: 2s - loss: 12.5668 - accuracy: 0.093 - ETA: 2s - loss: 12.5457 - accuracy: 0.093 - ETA: 2s - loss: 12.5573 - accuracy: 0.095 - ETA: 2s - loss: 12.5468 - accuracy: 0.096 - ETA: 2s - loss: 12.5295 - accuracy: 0.096 - ETA: 2s - loss: 12.5427 - accuracy: 0.096 - ETA: 1s - loss: 12.5334 - accuracy: 0.096 - ETA: 1s - loss: 12.5266 - accuracy: 0.097 - ETA: 1s - loss: 12.5259 - accuracy: 0.097 - ETA: 1s - loss: 12.5222 - accuracy: 0.097 - ETA: 1s - loss: 12.5250 - accuracy: 0.097 - ETA: 1s - loss: 12.5264 - accuracy: 0.097 - ETA: 1s - loss: 12.5294 - accuracy: 0.097 - ETA: 1s - loss: 12.5381 - accuracy: 0.096 - ETA: 1s - loss: 12.5328 - accuracy: 0.097 - ETA: 1s - loss: 12.5348 - accuracy: 0.096 - ETA: 1s - loss: 12.5431 - accuracy: 0.096 - ETA: 1s - loss: 12.5414 - accuracy: 0.096 - ETA: 1s - loss: 12.5387 - accuracy: 0.096 - ETA: 1s - loss: 12.5329 - accuracy: 0.096 - ETA: 1s - loss: 12.5301 - accuracy: 0.096 - ETA: 1s - loss: 12.5168 - accuracy: 0.096 - ETA: 1s - loss: 12.5162 - accuracy: 0.096 - ETA: 0s - loss: 12.5258 - accuracy: 0.096 - ETA: 0s - loss: 12.5181 - accuracy: 0.097 - ETA: 0s - loss: 12.5322 - accuracy: 0.096 - ETA: 0s - loss: 12.5362 - accuracy: 0.096 - ETA: 0s - loss: 12.5408 - accuracy: 0.096 - ETA: 0s - loss: 12.5338 - accuracy: 0.097 - ETA: 0s - loss: 12.5330 - accuracy: 0.097 - ETA: 0s - loss: 12.5339 - accuracy: 0.097 - ETA: 0s - loss: 12.5356 - accuracy: 0.097 - ETA: 0s - loss: 12.5406 - accuracy: 0.097 - ETA: 0s - loss: 12.5457 - accuracy: 0.097 - ETA: 0s - loss: 12.5462 - accuracy: 0.097 - ETA: 0s - loss: 12.5389 - accuracy: 0.097 - ETA: 0s - loss: 12.5392 - accuracy: 0.097 - ETA: 0s - loss: 12.5395 - accuracy: 0.097 - ETA: 0s - loss: 12.5427 - accuracy: 0.097 - ETA: 0s - loss: 12.5392 - accuracy: 0.097 - ETA: 0s - loss: 12.5478 - accuracy: 0.097 - ETA: 0s - loss: 12.5480 - accuracy: 0.097 - ETA: 0s - loss: 12.5472 - accuracy: 0.097 - ETA: 0s - loss: 12.5460 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 14.1946 - accuracy: 0.031 - ETA: 2s - loss: 12.2593 - accuracy: 0.092 - ETA: 2s - loss: 12.3880 - accuracy: 0.097 - ETA: 2s - loss: 12.3507 - accuracy: 0.098 - ETA: 2s - loss: 12.3678 - accuracy: 0.098 - ETA: 2s - loss: 12.3379 - accuracy: 0.100 - ETA: 2s - loss: 12.3934 - accuracy: 0.097 - ETA: 2s - loss: 12.3844 - accuracy: 0.097 - ETA: 2s - loss: 12.3759 - accuracy: 0.099 - ETA: 2s - loss: 12.4115 - accuracy: 0.098 - ETA: 2s - loss: 12.3915 - accuracy: 0.098 - ETA: 1s - loss: 12.4061 - accuracy: 0.098 - ETA: 2s - loss: 12.4155 - accuracy: 0.098 - ETA: 2s - loss: 12.4273 - accuracy: 0.098 - ETA: 1s - loss: 12.4357 - accuracy: 0.098 - ETA: 1s - loss: 12.4507 - accuracy: 0.098 - ETA: 1s - loss: 12.4621 - accuracy: 0.098 - ETA: 1s - loss: 12.4654 - accuracy: 0.098 - ETA: 1s - loss: 12.4714 - accuracy: 0.097 - ETA: 1s - loss: 12.4945 - accuracy: 0.097 - ETA: 1s - loss: 12.4826 - accuracy: 0.097 - ETA: 1s - loss: 12.4882 - accuracy: 0.098 - ETA: 1s - loss: 12.4781 - accuracy: 0.098 - ETA: 1s - loss: 12.4763 - accuracy: 0.098 - ETA: 1s - loss: 12.4845 - accuracy: 0.098 - ETA: 1s - loss: 12.4860 - accuracy: 0.098 - ETA: 1s - loss: 12.4854 - accuracy: 0.098 - ETA: 1s - loss: 12.4879 - accuracy: 0.098 - ETA: 1s - loss: 12.5039 - accuracy: 0.098 - ETA: 1s - loss: 12.5008 - accuracy: 0.098 - ETA: 1s - loss: 12.5060 - accuracy: 0.098 - ETA: 1s - loss: 12.5051 - accuracy: 0.098 - ETA: 1s - loss: 12.5146 - accuracy: 0.097 - ETA: 0s - loss: 12.5156 - accuracy: 0.097 - ETA: 0s - loss: 12.5048 - accuracy: 0.097 - ETA: 0s - loss: 12.5074 - accuracy: 0.097 - ETA: 0s - loss: 12.5120 - accuracy: 0.097 - ETA: 0s - loss: 12.5081 - accuracy: 0.097 - ETA: 0s - loss: 12.5130 - accuracy: 0.098 - ETA: 0s - loss: 12.5220 - accuracy: 0.097 - ETA: 0s - loss: 12.5170 - accuracy: 0.097 - ETA: 0s - loss: 12.5185 - accuracy: 0.097 - ETA: 0s - loss: 12.5298 - accuracy: 0.097 - ETA: 0s - loss: 12.5243 - accuracy: 0.097 - ETA: 0s - loss: 12.5235 - accuracy: 0.097 - ETA: 0s - loss: 12.5289 - accuracy: 0.097 - ETA: 0s - loss: 12.5211 - accuracy: 0.097 - ETA: 0s - loss: 12.5334 - accuracy: 0.097 - ETA: 0s - loss: 12.5296 - accuracy: 0.097 - ETA: 0s - loss: 12.5339 - accuracy: 0.097 - ETA: 0s - loss: 12.5375 - accuracy: 0.097 - ETA: 0s - loss: 12.5426 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 13.1872 - accuracy: 0.125 - ETA: 2s - loss: 12.6457 - accuracy: 0.099 - ETA: 2s - loss: 12.4898 - accuracy: 0.097 - ETA: 2s - loss: 12.5068 - accuracy: 0.097 - ETA: 2s - loss: 12.4515 - accuracy: 0.100 - ETA: 2s - loss: 12.4463 - accuracy: 0.100 - ETA: 2s - loss: 12.4450 - accuracy: 0.100 - ETA: 2s - loss: 12.4374 - accuracy: 0.101 - ETA: 2s - loss: 12.4633 - accuracy: 0.100 - ETA: 2s - loss: 12.4591 - accuracy: 0.101 - ETA: 2s - loss: 12.4718 - accuracy: 0.102 - ETA: 1s - loss: 12.4871 - accuracy: 0.101 - ETA: 1s - loss: 12.4943 - accuracy: 0.100 - ETA: 1s - loss: 12.5118 - accuracy: 0.099 - ETA: 1s - loss: 12.5143 - accuracy: 0.098 - ETA: 1s - loss: 12.5150 - accuracy: 0.099 - ETA: 1s - loss: 12.5153 - accuracy: 0.099 - ETA: 1s - loss: 12.5317 - accuracy: 0.098 - ETA: 1s - loss: 12.5163 - accuracy: 0.098 - ETA: 1s - loss: 12.5203 - accuracy: 0.098 - ETA: 1s - loss: 12.5379 - accuracy: 0.098 - ETA: 1s - loss: 12.5432 - accuracy: 0.097 - ETA: 1s - loss: 12.5465 - accuracy: 0.097 - ETA: 1s - loss: 12.5486 - accuracy: 0.097 - ETA: 1s - loss: 12.5467 - accuracy: 0.097 - ETA: 1s - loss: 12.5359 - accuracy: 0.098 - ETA: 1s - loss: 12.5446 - accuracy: 0.098 - ETA: 1s - loss: 12.5474 - accuracy: 0.098 - ETA: 1s - loss: 12.5544 - accuracy: 0.098 - ETA: 1s - loss: 12.5511 - accuracy: 0.097 - ETA: 1s - loss: 12.5534 - accuracy: 0.097 - ETA: 0s - loss: 12.5524 - accuracy: 0.097 - ETA: 0s - loss: 12.5526 - accuracy: 0.097 - ETA: 0s - loss: 12.5512 - accuracy: 0.097 - ETA: 0s - loss: 12.5435 - accuracy: 0.097 - ETA: 0s - loss: 12.5448 - accuracy: 0.097 - ETA: 0s - loss: 12.5426 - accuracy: 0.097 - ETA: 0s - loss: 12.5358 - accuracy: 0.097 - ETA: 0s - loss: 12.5324 - accuracy: 0.097 - ETA: 0s - loss: 12.5298 - accuracy: 0.097 - ETA: 0s - loss: 12.5219 - accuracy: 0.097 - ETA: 0s - loss: 12.5198 - accuracy: 0.097 - ETA: 0s - loss: 12.5279 - accuracy: 0.097 - ETA: 0s - loss: 12.5261 - accuracy: 0.097 - ETA: 0s - loss: 12.5282 - accuracy: 0.097 - ETA: 0s - loss: 12.5313 - accuracy: 0.097 - ETA: 0s - loss: 12.5337 - accuracy: 0.097 - ETA: 0s - loss: 12.5433 - accuracy: 0.097 - ETA: 0s - loss: 12.5445 - accuracy: 0.097 - ETA: 0s - loss: 12.5490 - accuracy: 0.097 - ETA: 0s - loss: 12.5432 - accuracy: 0.097 - 3s 1ms/step - loss: 12.5417 - accuracy: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2134109f908>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)]\n",
    "    )\n",
    "\n",
    "#model.compile(optimizer='adam',loss='SparseCategoricalCrossentropy',metrics='Accuracy')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='SparseCategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T18:12:21.998006Z",
     "start_time": "2021-03-05T18:12:21.458409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 9.355703e-35, 0.000000e+00, 0.000000e+00,\n",
       "       1.000000e+00, 0.000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss,test_acc=model.evaluate(test_images,test_labels,verbose=3)\n",
    "\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = probability_model.predict(test_images)\n",
    "\n",
    "predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T18:12:54.738400Z",
     "start_time": "2021-03-05T18:12:54.726433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:53:50.208845Z",
     "start_time": "2021-03-05T17:53:50.200866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUC',\n",
       " 'Accuracy',\n",
       " 'BinaryAccuracy',\n",
       " 'BinaryCrossentropy',\n",
       " 'CategoricalAccuracy',\n",
       " 'CategoricalCrossentropy',\n",
       " 'CategoricalHinge',\n",
       " 'CosineSimilarity',\n",
       " 'FalseNegatives',\n",
       " 'FalsePositives',\n",
       " 'Hinge',\n",
       " 'KLD',\n",
       " 'KLDivergence',\n",
       " 'LogCoshError',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " 'Mean',\n",
       " 'MeanAbsoluteError',\n",
       " 'MeanAbsolutePercentageError',\n",
       " 'MeanIoU',\n",
       " 'MeanRelativeError',\n",
       " 'MeanSquaredError',\n",
       " 'MeanSquaredLogarithmicError',\n",
       " 'MeanTensor',\n",
       " 'Metric',\n",
       " 'Poisson',\n",
       " 'Precision',\n",
       " 'PrecisionAtRecall',\n",
       " 'Recall',\n",
       " 'RecallAtPrecision',\n",
       " 'RootMeanSquaredError',\n",
       " 'SensitivityAtSpecificity',\n",
       " 'SparseCategoricalAccuracy',\n",
       " 'SparseCategoricalCrossentropy',\n",
       " 'SparseTopKCategoricalAccuracy',\n",
       " 'SpecificityAtSensitivity',\n",
       " 'SquaredHinge',\n",
       " 'Sum',\n",
       " 'TopKCategoricalAccuracy',\n",
       " 'TrueNegatives',\n",
       " 'TruePositives',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'binary_accuracy',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_accuracy',\n",
       " 'categorical_crossentropy',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kl_divergence',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'serialize',\n",
       " 'sparse_categorical_accuracy',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'sparse_top_k_categorical_accuracy',\n",
       " 'squared_hinge',\n",
       " 'top_k_categorical_accuracy']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tensorflow.keras.optimizers)\n",
    "\n",
    "dir(tensorflow.keras.losses)\n",
    "\n",
    "dir(tensorflow.keras.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:53:50.514280Z",
     "start_time": "2021-03-05T17:53:50.499324Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T17:53:50.986084Z",
     "start_time": "2021-03-05T17:53:50.838439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:759 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:409 update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py:90 decorated\n        update_op = update_state_fn(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:176 update_state_fn\n        return ag_update_state(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:612 update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:3208 accuracy  **\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 10) and (32, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-4162f1ec0345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:759 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:409 update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\utils\\metrics_utils.py:90 decorated\n        update_op = update_state_fn(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:176 update_state_fn\n        return ag_update_state(*args, **kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:612 update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py:3208 accuracy  **\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n    C:\\Users\\Harish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 10) and (32, 1) are incompatible\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
